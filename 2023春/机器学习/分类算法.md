### 分类算法



#### 1. KNN算法

##### 1.1 介绍

> K近邻（K-Nearest Neighbor, KNN）是一种最经典和最简单的*有监督学习*方法之一。
>
> 定义：如果一个样本在特征空间中的K个最相似（即特征空间中最近邻）的样本中的大多数属于一个类别，那么该样本也属于此类别
>
> 其核心思想：用你的邻居来推断出你的类型

##### 1.2 API

```python
estimator = sklearn.neighbors.KNeighbarsClassifier(n_neighbors=5,algorithm='auto')
```

##### 1.3 实例

在这里以鸢尾花数据集为案例。

![image-20230224171201085](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230224171201085.png)





#### 2. 朴素贝叶斯算法

##### 2.1 介绍

> 贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而朴素朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法。
>
> 朴素：假设各个特征之间是独立的。

##### 2.2 API

```python
sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
	- alpha:拉普拉斯平滑系数(防止概率出现0的情况)
```

##### 2.3 总结

- 优点:对缺失数据不太敏感，算法比较简单，速度快。常用于**文本分类**
- 缺点:由于使用了样本特征独立性的假设，所以如果特征属性有关联的时候其效果不佳 



#### 3. 决策树

##### 3.1 先导

- 信息：信息的价值在于能够消除不确定性

- 信息熵：传输(表示)该信息需要的bit数(极限)

  - 我的理解是：至少需要多少bit的信息才能消除其不确定性，例如抛硬币的信息熵为1，那么我们需要用1bit的数据来消除是正面还是反面的不确定性；又假设四张牌分别为1234，摸一张牌，这张牌形的信息熵为2，也就是说我们需要用2bit来消除其不确定性。

  <img src="https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230228162029896.png" alt="image-20230228162029896" style="zoom:50%;" />

- 信息量：等可能事件信息熵

- 信息增益：得知了这个信息之后，信息熵减少了多少。

##### 3.2 原理

我们需要选择逐步选择信息增益最大的特征，放在决策树的顶部来尽量减少决策的次数，那么因此我们采用信息增益的方式来构建决策树。

<img src="https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230228162539256.png" alt="image-20230228162539256" style="zoom:50%;" />

##### 3.3 API

```python
sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)

	- criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’
	- max_depth:树的深度大小
	- random_state:随机数种子
	- 其中会有些超参数：max_depth:树的深度大小


```

##### 3.4 总结

- 优点：可视化--可解释能力强
- 缺点：调参较为复杂，若Depth设置不当容易出现过拟合和欠拟合

---







#### 4.随机森林

##### 4.1 介绍

> 在用决策树的时候，若机器对训练集特征学习过于拟合，则会出现过拟合的问题，俗称学傻了。这个时候我们就可以用随机森林的方式去解决，随机森林其实是一种大众决策的方式

##### 4.2 核心

- 随机：
  - 样本集随机：bootstrap
  - 特征随机：从M个特征中抽取m个特征
- 森林：
  - 多个决策树同时估计，在各个树产生的结果中选择结果。

