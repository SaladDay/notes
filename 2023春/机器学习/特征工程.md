### 特征工程

#### 1. 为什么需要特征工程？

> 机器学习领域的大神Andrew Ng(吴恩达)老师说“Coming up with features is difficult, time consuming, requires expert knowledge. “Applied machine learning” is basically feature engineering. ”
>
> 注：业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

#### 2.什么是特征工程？

特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上能够发挥更好的作用。

#### 3.流程及工具

 	1. Prepare your data--->pandas
 	2. Feature engineering--->sklearn
 	3. Learn from your data---->sklearn

#### 4.特征提取

> 目的：将任意数据(如文本或图像)转化为可用于机器学习的数字特征
>
> - 字典特征提取
> - 文本特征提取
> - 图像特征提取（深度学习）
>
> `API:sklearn.feature_extraction`

##### 4.1 字典（类）特征提取

```python
sklearn.feature_extraction.DicVectorizer()
	- DicVectorizer.fit_transform(X)		#X为字典或者包含字典的迭代器； 返回sparse矩阵
    - DicVectorizer.get_feature_names_out()	#返回属性名称
```

###### 4.1.1应用

我们对一下的数据进行特征提取。

```bash
[{'city': '北京','temperature':100}
{'city': '上海','temperature':60}
{'city': '深圳','temperature':30}]
```

在经过以下代码处理后：

```python
from sklearn.feature_extraction import DictVectorizer
def dict_demo():
"""
对字典类型的数据进行特征抽取
:return: None
"""
data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60},
{'city': '深圳','temperature':30}]
# 1、实例化一个转换器类
transfer = DictVectorizer(sparse=False)
# 2、调用fit_transform
data = transfer.fit_transform(data)
print("返回的结果:\n", data)
# 打印特征名字
print("特征名字：\n", transfer.get_feature_names())
return None
```

能够返回一个稀疏矩阵：

```bash
返回的结果:
(0, 1) 1.0
(0, 3) 100.0
(1, 0) 1.0
(1, 3) 60.0
(2, 2) 1.0
(2, 3) 30.0
特征名字：
['city=上海', 'city=北京', 'city=深圳', 'temperature']
```

而此稀疏矩阵对应的二位数据矩阵为：

(通过`data.toarray()`可将其转化)

```python
返回的结果:
[[ 0. 1. 0. 100.]
[ 1. 0. 0. 60.]
[ 0. 0. 1. 30.]]
特征名字：
['city=上海', 'city=北京', 'city=深圳', 'temperature']
```

###### 4.1.2 分析

我们将其数据做one-hot编码处理，避免数字排序影响到机器学习的结果。



---







##### 4.2 文本特征提取

```python
sklearn.feature_extraction.text.CountVectorizer(stop_word=[])	#实例化转换器
	- CountVectorizer.fit_transform(X)			#X：文本或者包含文本的迭代器 返回：sparse Vector
    - CountVectorizer.get_feature_names_out		#返回单词列表

```

###### 4.2.1应用

我们对以下数据进行特征提取：

```python
["life is short,i like python",
"life is too long,i dislike python"
```

在经过以下代码后：

```python
from sklearn.feature_extraction.text import CountVectorizer
def text_count_demo():
"""
对文本进行特征抽取，countvetorizer
:return: None
"""
data = ["life is short,i like like python", "life is too long,i dislike
python"]
# 1、实例化一个转换器类
# transfer = CountVectorizer(sparse=False)
transfer = CountVectorizer()
# 2、调用fit_transform
data = transfer.fit_transform(data)
print("文本特征抽取的结果：\n", data.toarray())
print("返回特征名字：\n", transfer.get_feature_names())
return None
```

返回结果：

```python
文本特征抽取的结果：
[[0 1 1 2 0 1 1 0]
[1 1 1 0 1 1 0 1]]
返回特征名字：
['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too'
```

如果我们输入的数据替换为英语，会发现结果不可用。

我们需要对中文用“空格”进行分割分词处理。

==但是！==

我们得到的文本特征如果用于分类，会发现其标识度不够，不能有效反应文章的类型。

###### 4.2.2 改进---Tf-idf文本特征提取

- TF-IDF的主要思想是：如果某个词语在一篇文章中出现的多，而在其他文章中出现的次数很少，那么我们可以认为此词语具有很好的类别区分能力，适合用来分类。
- TF-IDF的作用：用以评估一**词语**对于**一个文件集或一个语料库**中的其中**一份文件**的重要程度。

> 注：假如一篇文件的总词语数是100个，而词语"非常"出现了5次，那么"非常"一词在该文件中的词频就是 5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现"非常"一词的文件数。所 以，如果"非常"一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg （10,000,000 / 1,0000）=3。最后"非常"对于这篇文档的tf-idf的分数为0.05 * 3=0.15





---



#### 5.特征预处理

##### 5.1 什么是特征预处理:

通过一些转换函数将特征数据转换成更加适合算法模型的特征数据

需要将数值型数据==无量纲化==(包括归一化和标准化)

##### 5.2 特征预处理API

```python
sklearn.preprocessing
```

##### 5.3 为什么需要归一化/标准化?

特征的单位(或者相对大小)相差较大,容易影响(支配)目标结果,使得一些算法无法学习到其他的特征

---



#### 6.特征降维

> 降维指的是：在某些限定条件下，降低随机变量的个数，得到一组”不相关“的主变量的过程

##### 6.1 为什么要降维？

在学习的时候，我们都是利用特征来学习，而特征的数量往往都是很多的，如果特征之间的相关性比较强或者冗余特征比较多，会影响学习算法的效率。

##### 6.2 特征降维的方法：

主要是两大类，一类是特征选择，一类是主成分分析（PAC）

###### 6.2.1 特征选择

1. 降低低方差特征：一些特征他们在样本中的共同性很高，比如说：在辨别猫的品种的任务中，某个特征为有四只腿。这个特征对于这个任务而言是没有任何作用的，所有样本的属性值都是true，那么这个特征是可以被清除的。
2. 相关系数：某两个特征之间相关度特别高，其实可以用一个特征表征另一个特征，比如说：一个特征为X，另一个特征为X+1,这两个特征的相关系数明显是1，那么这两个特征是可以去除一个的。

###### 6.2.2 主成分分析（PAC）

利用PAC算法，在尽量保持数据信息的情况下进行降维，是一种很有效的去处冗余数据的算法。

​	



