# 从似然到交叉熵：极大似然、对数似然、负对数似然、Pytorch



> 此篇我们将从似然的定义出发，逐步推导出负对数似然，再与交叉熵对比。
>
> 全篇说人话。
>
> 同时会介绍pytorch的部分API。

## 1. Definition of max likelihood

> 在很多时候，Probability与Likelihood是相同的，但是在统计学中，两者是有区别的。

我们想象两个事物，一个是模型、是理想的；一个是事实、是现实的。

事实的期望与模型是相当的，但是事实具有现实性，因此其与模型是不尽相同的。

Probability是在模型已知的情况下，事实发生的概率。

>  例如，已经有了一个硬币(投硬币的模型已知，当然是用无数次事实而得来的**近似的期望**计算而来)，我们去估计还没有发生的事实(投出的硬币是正是反)。

Likelihood则恰恰相反，事实已经发生了(可以发生一次可以发生无数次)，我们需要去估计这个模型(在机器学习中则是估计模型的参数)。

> 例如，我们已经投了十次硬币，六次是正面的，我们去估计投硬币的模型(在机器学习则可以表现为，估计硬币正面朝上的概率θ)

他们表达的其实是同一件事，只是立足于不同的角度。因此两者在数值上是相同的，但在意义上是不同的。
$$
P(X|θ) = L(θ|X)
$$

## 2. The  process from MLE to CrossEntropyLoss

### 2.1. Maximum likelihood estimation(MLE)

> 用一句话概括似然：
>
> 现实下已经发生了这件事，他在理论世界中对应某个A模型的可能性 叫做A模型的似然值。

当我们理解了似然，自然极大似然也就出来了。我们就是想求得具有最大似然值的模型，也就是其最有可能性对应的模型。

给定一个概率模型$\mathcal{D}$，已知其概率密度函数(离散型为概率分布函数)为$f_θ\mathcal{D}$，其中θ是模型的参数。

我们可以从这个模型中抽出一个具有n个值的采样（batch_size）$(X_1,X_2,X_3,...,X_n)$，利用$f_θ\mathcal{D}$计算其似然。
$$
L(θ|X_1,X_2,...,X_n) = P(X_1,X_2,...,X_n|θ)
$$

由于机器学习基本假设——样本独立，可以将(2)式推导：
$$
P(X_1,X_2,...,X_n)=\prod_{i=1}^n f_θ(X_i)
$$

此式中，我们能够看出我们只要一旦获得一组采样，我们就能获得一个关于θ的似然函数。

从数学上来说，我们可以在所有的θ中取到一个θ，使其带入似然函数中时，使似然函数最大，那么这个θ就是极大似然估计。即
$$
\hat{θ} =\arg\max\limits_{θ}{L(θ|X_1,X_2,...,X_n)}.
$$

#### E2.1.1 MLE in Binary problem

考虑二分类问题。
$$
\hat{y}_θ = \sigma(t_θ(x))
$$
此处，$\hat{y}_θ$为预测为正样本的概率。$t_θ(x)$是带有参数的线性变换，$\sigma(x)$为非线性变换函数。

特别注意一点，在此处$\sigma(t_θ(x))$对应的是模型的概率密度函数$f_θ\mathcal{D}$。

那么，
$$
\begin{eqnarray}
L(θ|X_1,X_2,...,X_n)
=&P(X_1,X_2,...,X_n|θ) \nonumber \\
=&\prod_{i=1}^n (\hat{y}_{θ,i})^{y_i}(1-\hat{y}_{θ,i})^{1-y_i}
\end{eqnarray}
$$
此式中，$y_i$代表第i个样本真实的情况，0代表反例，1代表真例。显然，当真实为真例时，前项作用；反之，后项作用。



### 2.2. Log-Likelihood

为方便计算，我们将Likelihood加了个对数即变为了Log-Likelihood

#### E2.2.1 Log-Likelihood in Binary problem

$$
\begin{eqnarray}
logL(θ|X_1,X_2,...,X_n)&=&logP(X_1,X_2,...,X_n|θ) \nonumber    \\
~&=&\sum_{i=1}^n (y_ilog(\hat{y}_{θ,i})+(1-y_i)log(1-\hat{y}_{θ,i}))   \\

\end{eqnarray}
$$



### 2.3.Negative Log-Likelihood 

> 由于此小结过于简短，我们不分出额外的E2.3.2，将Negative Log-Likelihood in Binary problem放在一起解释

在机器学习里，我们往往将损失函数作为需要优化的目标，将具有最小损失函数的模型称为optimum。

因此我们习惯于在(7)式上加上负号，使其变为，
$$
logL(θ|X_1,X_2,...,X_n) =-\sum_{i=1}^n (y_ilog(\hat{y}_{θ,i})+(1-y_i)log(1-\hat{y}_{θ,i}))
$$
这里简单的总结一下，我们当初的目标是获得最大似然$\hat{θ}$，现在Negetive Log-Likelihood其实并没有背弃当初的目标，只是做了变换，虽然其极值发生了改变，但是其极值点的θ没有发生变化，依旧是极大似然估计$\hat{θ}$。

### 2.4. CrossEntropyLoss

其与Negetive Log-Likelihood在数学表达式上很相似，只不过是从不同的角度去理解这个问题。

交叉熵是信息论中衡量两个概率分布(模型)之间差异的指标。

负对数似然是通过概率论工具推导而来。

其实要较真的话，两者之间的差别还是有的，例如：在交叉熵中，log的底数一定是2是不可以改变的，因为其为熵在信息论中的定义。而在负对数似然中，log的底数是多少都无所谓，log存在的意义只不过是将连乘变为连加。



## 3. Generalize to Multiclass Classification

上面讨论的是二分类问题，而往往我们遇到的是多分类问题。
$$
logL(θ|\mathcal{D})=log(\mathcal{D}|θ) = \sum_{i=1}^n log(\hat{y}_{θ,i}^{(y_i)})
$$
注意，在此式，$log(\hat{y}_{θ,i}^{(y_i)})$中上标$(y_i)$表示在C Classes分类问题中，仅对真实类别位置所对应概率值求对数。

用向量解释为：$\hat{y}_{θ,i}$为一维向量，长度为C。$\hat{y}_{θ,i}^{(y_i)}$则为在$\hat{y}_{θ,i}$中取出第$y_i$个。

为统一表达，我们将二分类问题也转化为此方式方式：
$$
logL(θ|X_1,X_2,...,X_n) = -\sum_{i=1}^n (log(\hat{y}_{θ,i}^{(y_i)})\\
其中，\hat{y}_{θ,i}^{(1)} = \hat{y}_{θ,i},\hat{y}_{θ,i}^{(0)}=1-\hat{y}_{θ,i}
$$

## 4. PytorchAPI

> 我会将PytorchAPI与上述推导过程对比说明。

#### 4.1 logSoftMax

> *CLASS*torch.nn.LogSoftmax(*dim=None*)

需要输入的是未经过normalized的输出（logits）。
$$
LogSoftmax(x_i) = log(\frac{exp(x_i)}{\sum_{j}exp(x_j)})
$$
我们将(9)式变为:
$$
logL(θ|\mathcal{D})=\sum_{i=1}^n log(\hat{y}_{θ,i}^{(y_i)})=\sum_{i=1}^n(log\,\hat{y}_{θ,i})^{(y_i)}
$$
这个函数的作用其实就是求出$log\,\hat{y}_{θ,i}$的部分。

对于多分类问题，得到的是normalized并且求了对数的输入，输出依旧是一个向量，也称log-probabilities。

#### 4.2 NLLLOSS

> torch.nn.NLLLoss(*weight=None*, *size_average=None*, *ignore_index=- 100*, *reduce=None*, *reduction='mean'*)

需要输入的是log-probabilities。input.shape = (batch_size,C)。

由于具体算法的实现，因此target应该属于range(C)。

> 因为其是用数组索引来实现**仅取真实类别位置所对应概率值**的

$$
\begin{gather*}
\mathcal{l}(x,y)=\{l_1,l_2,...,l_n\}^T\\
其中\,\,l_i= -w^{(y^i)}x_i^{(y^i)}
\end{gather*}
$$

其中x是input，y是target，n是batch_size，w(weight)是每一个分类的权重。当你有一个不平衡的训练集时，设置w是很有用的。

如果参数中的reduction设置为：

- none，直接返回l(x,y)，向量，长度为batch_size
- mean，返回其平均值，标量
- sum，返回其和，标量。**其实我们前三小节用的都是sum**

#### 4.3 CROSSENTROPYLOSS

> torch.nn.CrossEntropyLoss(*weight=None*, *size_average=None*, *ignore_index=- 100*, *reduce=None*, *reduction='mean'*, *label_smoothing=0.0*)

需要输入的是未经过normalized的输出（logits）。input.shape=(batch_size,C)

由于具体算法的实现，因此target应该属于range(C)。
$$
\begin{gather*}
\mathcal{l}(x,y)=\{l_1,l_2,...,l_n\}^T\\
其中\,\,l_i= -w^{(y^i)}log\frac{exp(x_n^{(y^n)})}{\sum_{c=1}^Cexp(x_n^{(c)})}
\end{gather*}
$$
如果参数中的reduction设置为：

- none，直接返回l(x,y)，向量，长度为batch_size
- mean，返回其平均值，标量
- sum，返回其和，标量。**其实我们前三小节用的都是sum**

为加深印象，同样将其与我们在前三节中的推导对应。

由于之前我们用的都是sum的loss，且我们没有设置weight，即为
$$
loss= \sum_{i=1}^n l_n=\sum_{i=1}^n log\frac{exp(x_n^{(y^n)})}{\sum_{c=1}^Cexp(x_n^{(c)})}
$$
与(9)式对比，发现，(9)式中的$\hat{y}_{θ,i}$即为此式中的$\frac{exp(x_n)}{\sum_{c=1}^Cexp(x_n^{(c)})}$。

最后，其实CROSSENTROPYLOSS也就等于在NLLLOSS前加上logSoftMax。

> Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.













