### 第一章 绪论

1. ==假设空间==hypothesis space指的是 可能的解决方案或模型的集合。假设空间的具体形式取决于所使用的机器学习算法和问题的性质。在监督学习中，假设空间通常由参数化模型表示，例如线性回归、决策树、神经网络等，每个模型都可以有一组参数来表示。

   在假设空间中，每一个可能得假设代表了一个可能的解决方案或模型，学习的目标是尽可能的让这个解决方案或模型fit训练集。

2. 而通常会有多个解决方案或模型fit训练集，这些fit训练集的解决方案或模型的集合便称为==版本空间==。如何去挑选一个更好的模型呢？这就需要归纳（学习或者说是选择具体模型）的过程有一定的==归纳偏好==。

   1. 这里需要注意一个东西，学习算法（模型）的选择也是一种归纳偏好，其直接决定了学习器能否取得好的性能。
   2. 奥卡姆剃刀：简单的更好。这是基本原则。
   3. 没有免费午餐定理：一个模型在这里好不代表他到另外的地方依旧好

### 第二章 模型的评估与选择

1. 评估方法：

   我们通常使用测试集上的“测试误差”来近似泛化误差，因此我们需要尽可能保证测试集和训练集互斥（训练集出现过的在测试集不出现）

   下面是几种常见的分割方法：

   1. 留出法
      - 直接将数据集划分为两个互斥集合
      - 尽量不改变数据的原始分布
   2. 交叉验证法
      - 将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的 并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的君 子，k最常用的取值是10
   3. 自助法
      - 对数据集*D* ==有放回==采样 *m* 次 得到训练集*D’*,用*D\D’* 做测试集。
      - 由于改变了数据的分布，有可能引入误差，尽可能只在数据量很小的情况下使用。

2. 性能度量

   1. 回归任务往往使用均方误差MSE

   2. 分类任务一般使用错误率和精度（分对样本占样本总数的比例）

   3. 其他性能指标，在不同情况下会不同的用到
      1. 查全率：查的全不全，正例有没有全部被预测出来

      2. 查准率：查的准不准，预测出来是真例中的真正例的比率

      3. 统计真实标记和预测结果的组合可以得到“混淆矩阵”，根据混淆矩阵的结果可以画出P-R曲线。平衡点是曲线上“查准率=查全率”（==因为我想要两者都高==）时的取值，可用来用于度量P-R曲线有交叉的分类器性能高低。

         - P-R曲线绘制步骤：

           - 原始数据：

           - 

           - | 样本 | 预测概率 | 真实标签 |
             | ---- | -------- | -------- |
             | 1    | 0.8      | 正例     |
             | 2    | 0.6      | 正例     |
             | 3    | 0.3      | 负例     |
             | 4    | 0.2      | 正例     |
             | 5    | 0.7      | 负例     |

             根据不同阈值划分“预测的标签”：

             | 阈值 | 精确率 | 召回率 |
             | ---- | ------ | ------ |
             | 0.2  | 0.4    | 1.0    |
             | 0.3  | 0.5    | 0.67   |
             | 0.6  | 0.67   | 0.67   |
             | 0.7  | 0.5    | 0.33   |
             | 0.8  | 0.67   | 0.33   |

           - 以精确率和召回率为坐标画图。

      4. F1度量（比P-R曲线更常用）：

         ![image-20230603152617796](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230603152617796.png)

      5. ROC和AUC

         - 与P-R曲线很类似，只不过把横纵坐标的P-R改为了TPR（真正例率），FPR（假正例率）。
         - 绘制过程也一模一样。
         - AUC是ROC曲线下的面积，用来衡量二分类模型的性能。

      6. 代价敏感错误率和代价曲线

         - 在现实情况下，不同类型的错误所造成的后果很可能不同。
         - 代价敏感错误率：![image-20230603153140990](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230603153140990.png)
         - 代价曲线：在非均等代价中，ROC曲线不能直接反映出模型的性能，而代价曲线可以。
           - 画法基本相同，还是不同阈值为不同的点。其中纵坐标改为代价（需要计算而来）。横坐标可以是准确率或者是召回率等等。

3. 比较检验看不懂，好像是用统计学的方式去比较两个模型的优劣

4. 偏差和方差，在我看来，偏差就是模型拟合训练集的能力，方差就是模型拟合测试集的能力。噪声是数据集本身的错误，对特征的辨别造成了干扰，是学习问题本身的难度。

### 第三章 线性模型

1. 线性回归：

   - 目标：学得一个线性模型以尽可能准确地预测实值输出标记
   - 模型求解：基于最小化MSE来进行模型求解的方法叫做==最小二乘法==。

   $$
   \underset{(w,b)}{\arg\max} \sum_{i=1}^{m}(f(x_i)-y_i)^2
   $$

   - 在线性回归中，最小二乘法就是试图找到一条直线，使得所有样本到直线上的欧氏距离之和最小。
   - 这里涉及到一个问题，由于线性回归的损失函数比较简单，是可以由直接求导得到解析解的，不管是一个特征或者是多个特征，只要对不同的w，b进行求导即可。但是如果矩阵的转置乘以自身不可逆（奇异矩阵），则这个参数没有解析解，就要用数值优化方法（梯度下降等）来逼近解。
     - 需要强调的是，在实际情况下，有些时候我们即使存在解析解，我们依然会使用梯度下降（计算快，受限少，通用）。

2. 逻辑回归：

   - 一种线性分类模型，使用一个线性函数将输入特征的加权和映射到一个概率值。通常使用逻辑函数（如Sigmoid函数）对线性加权和进行非线性变换，将结果限制在0到1之间，表示样本属于某个类别的概率。

   - 注意，在处理二分类问题的时候，其与单层感知机（神经元）几乎一致。

   - 损失函数：使用交叉熵，度量逻辑回归模型的预测值与实际标签之间的差异。
     $$
     L(p, y) = -y * log(p) - (1-y) * log(1-p)
     $$
     

3. 线性判别分析：

   - 是一种经典的监督学习算法，用于降维和分类问题。它通过对数据进行线性变换，将高维特征映射到低维空间，同时最大化类别间的可分性。
   - 给定训练集，将样例都投影到一个直线（超平面），使得同类尽可能近，不同类尽可能远。在数学上，可以让同类样例投影点的协方差尽可能的小，而使得不同类的投影点的协方差尽可能的大，同时考虑二者即可得到欲最大化的目标。
   - 优化方法：一般是用矩阵论、广义瑞利商来进行，其实也可以用梯度下降

4. 多分类学习

   我们可以将二分类学习器改造成多分类学习器，但这个其实就很像神经网络了。一般来说，我们用的多的还是将二分类学习器集成来实现多分类学习。常见的策略OvO,OvR,MvM。

   ![image-20230603163811562](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230603163811562.png)

   1. OvO：

      比较简单，两两组合，最后投票生成预测值。

   2. OvR：

      也比较简单，每次将一个作为正例；Rest作为负例。看哪个分类器预测为正例，则预测结果为其。

      如果所有二分类学习器都分为反例的话，一般是拒绝分类；

      如果有多个二分类学习器分为正例的话，一般是选择最高置信度的。

   3. MvM：

      有点难。其主要的思想是，将多个分类目标进行二进制编码，对编码的每一位进行预测。

      采用“距离”的概念来判断最终的预测结果，每一个类别根据其每一位可以得到一个“海明距离、欧氏距离”，而我们可以看输入样例距离哪个最近来选择最终的预测。

      - ECOC编码对分类器错误有一定容忍和修正能力，编码越长、纠错能力越强
      - 对同等长度的编码，理论上来说，任意两个类别之间的编码距离越远，则纠错能力越强

5. 欠采样和过采样（再缩放）

   - 欠采样很好理解，少拿点呗。但EasyEnsemble的做法是不浪费样本，将正例（多类例）分为多份，使得每一份的数量和反例相当，然后从反例中随机采样，然后组成一份，用这一份训练一个基分类器，最后投票表决。
   - 过采样的话，基本方法1.重复采样；2.插值法，先锁定一个正例（少的那个），然后基于欧氏距离找到附近最近的一个点，然后在两个点之间随机采样一个点。





### 第四章 决策树

1. 在建树的时候，有三种情况需要返回：

   1. **纯度达标**，当前节点包含样本全部属于一个类别
   2. **达到预设的最大深度**
   3. **节点包含的样本数目小于预设阈值**，会选择最多的，然后自己变成叶子结点

2. 划分选择

   我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即“纯度”越来越高，划分选择往往有以下几种方式：

   1. 信息增益

      基于信息熵，$Ent(D) = -\sum^{|y|}_{k=1}p_klog_2p_k$,信息增益就是信息熵的增益

   2. 增益率

      假设我们把编号也作为候选划分属性，我们会发现编号的信息增益为0.998，原因很显然，编号将每一个样本都分为了一个组，每一组的信息熵为0。

      因此提出了增益率，加入了一个分母来偏好取值较少的特征。

   3. 基尼系数
      $$
      Gini(D) = \sum_{k=1}^{|y|}\sum_{k'!=k}p_kp_{k'}
      $$
      直观来说，基尼系数就是随机在集合中取两个，两个不是一个类的期望。Gini越小说明其不是一个类的概率越小，说明越纯净。（信息熵也是越小越纯净）

   ----

   这里给一个基于信息熵构建决策树的例子：

   假设我们有一个二分类问题，要根据两个特征 "年龄" 和 "收入" 来预测一个人是否会购买某个产品。我们的训练集如下：
   样本   年龄    收入    购买
   1      青年    高      是
   2      青年    低      是
   3      中年    低      是
   4      老年    低      否
   5      老年    高      否

   首先，计算原始数据集的信息熵。在这个例子中，样本总数为5，其中2个样本属于"是"类别，3个样本属于"否"类别。因此，原始数据集的信息熵为：
   $$
   H(D) = -3/5*log_2(3/5)-2/5*log_2(2/5)=0.971
   $$
   

   接下来，我们计算每个特征的信息增益，并选择具有最大信息增益的特征作为根节点。首先，计算特征 "年龄" 的信息增益：

   - 当 "年龄" 为 "青年" 时，有2个样本属于"是"类别，没有样本属于"否"类别。因此,$H(D_{{青年}}) = 0$
   - 当 "年龄" 为 "中年" 时，有1个样本属于"是"类别，1个样本属于"否"类别。因此，$H(D_{{中年}}) = 1$
   - 当 "年龄" 为 "老年" 时，有0个样本属于"是"类别，2个样本属于"否"类别。因此，$H(D_{{老年}}) = 0$

   因此，“年龄”特征的信息增益为：
   $$
   Gain(D,年龄)=  H(D) - (2/5*0+1/5*1+2/5*0)=0.171
   $$
   同样方法计算其他两个特征的。

   ----

3. 剪枝处理

   此时引入了一个东西叫做“验证集”，他与测试集的区别主要是，测试集在训练的全程对于学习器来说是不可见的。而验证集的作用是辅助训练。

   1. 预剪枝

   在广义来说，最大深度限制、最小样本数限制、类别纯度限制、最小信息增益限制都是预剪枝的一种，书上说的是基于验证集的预剪枝，其实也就是在验证集上看看，如果划分了精度反而下降的话，那就不分了。

   好处是其可以提早结束建树，减小训练开销，也可以避免过拟合。坏处是这是一种贪心策略，会过早挺早划分，有些时候后续的划分可能会大大的提高精度，但都被舍弃了。

   2. 后剪枝

   先建树，建树结束以后再自底而上剪枝。策略和预剪枝差不多，也是对比验证集，精度有提升就合并。

   好处是泛化性能往往优于预剪枝决策树、最大限度地利用训练数据，达到的是全局最优解。坏处很明显，需要先建树，计算开销大。

4. 连续值处理

   把连续值排序，==两两取中间值==作为候选分割点。

   采用离散属性值方法，选取最优分割点使得信息增益最大。

   注意：与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。

5. 缺失值处理

   一般缺失值的处理方式1.去除样本2.去除特征3.众树填补4.平均值填补。

   但是这种方式在样本数少的情况下都不大合适。

   我们采取的是“人人有份“的方式。

   在寻找最优特征的时候我们不考虑缺失值。

   在分流数据的时候，正常值正常分；对于缺失值，我们按照不同分支的样本数将其乘不同的权重进行分割，然后分给各个节点。

