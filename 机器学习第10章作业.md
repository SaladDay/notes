# 机器学习第10章作业

​												20214865罗帆靖

###  编程实现k近邻分类器，在西瓜数据集3.0α上比较其分类边界与决策树边界之异同。

见附录。

其分类边界与决策树边界有以下异同：

1. 边界形状：K近邻分类的决策边界通常是非线性的，并且根据最近邻居的类别进行划分。边界的形状取决于所选择的K值和距离度量方法。决策树的边界可以是非线性的，但也可以是线性的。它通过划分特征空间，根据特征的阈值进行分类。
2. 计算复杂度：K近邻分类的计算复杂度相对较高，因为它需要计算未知样本与所有已知样本之间的距离，并找出最近的K个邻居。决策树的计算复杂度相对较低，因为它只需要对特征进行有限次比较和判断。
3. 数据要求：K近邻分类对数据的特征缩放和噪声比较敏感，因为距离度量会受到这些因素的影响。决策树相对较不受数据特征缩放和噪声的影响，因为它是基于特征值的比较而不是距离度量。
4. 对训练数据的依赖性：K近邻分类对于训练数据的分布和密度依赖性较高，因为它是通过最近邻居进行决策的。决策树对训练数据的分布和密度依赖性较低，因为它是基于特征空间的划分进行决策的。
5. 可解释性：决策树具有较好的可解释性，可以生成一系列简单易懂的规则来解释分类过程。K近邻分类的可解释性较差，因为它仅仅是基于最近邻居的投票来决定分类结果。

### 令err,err∗分别表示最近邻分类器与贝叶斯最优分类器的期望错误率，试证明：err∗≤err≤err∗(2−||||−1×err∗).

![image-20230528002245385](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230528002245385.png)

###  在对高维数据降维前应该先进行“中心化”，常见的方法是将协方差矩阵$XX^T$转换为$XHH^TX^T$，其中$H=I-\frac{m}{1}11^T$。试分析其效果。

![image-20230528002308058](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230528002308058.png)

###  降维中涉及的投影矩阵通常要求是正交的，试述正交、非正交投影矩阵用于降维的优缺点。

投影矩阵在降维过程中起着重要的作用，而正交和非正交投影矩阵具有不同的优缺点。正交投影矩阵通过保持数据的线性关系和消除冗余信息来进行降维，从而保留数据的结构和信息，并具有较强的解释性。然而，它可能无法捕捉到数据中的非线性特征和变化。相比之下，非正交投影矩阵可以捕捉到非线性关系，并保留更多的原始数据信息，但计算复杂度较高，并且可解释性较差。因此，在选择投影矩阵时，需要根据具体任务和数据特点进行权衡，以获得最佳的降维效果。13488248364 ljt1744736200