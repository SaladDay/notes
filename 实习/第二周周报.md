

## 工作总结

### 对图标进行了矢量图重绘。



![image-20230706145915523](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230706145915523.png)

![iShot_2023-07-05_10.12.51](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/iShot_2023-07-05_10.12.51.png)

### 阅读WebRTC源码

### 修改WebRTC编解码的模块，并在服务器端完成了打包编译

## 工作过程与进展

### 首先本周工作最关键的问题在于，修改webRTC原生的编解码器。

1. 最初的方案决定手改WebRTC源码，将小组完成的OPUS嵌入。

2. 在阅读WebRTC源码之后，发现其编码使用的正是OPUS，于是产生了有两种解决方案

   1. 不动WebRTC底层源码，从SDP信息上做手脚（Java层），但是客户端SDP发生的部分（引擎部分）代码是不开源的。所以考虑从服务端做手脚，在服务器转发的时候附加（修改）信息。但是目前服务器并且显式接受客户端信息，定位代码位置耗费时间。

   2. 改变WebRTC底层源码。【我做的是这一方向的努力】



于是开始看源码细节

```c++

absl::optional<AudioEncoderOpusConfig> AudioEncoderOpusImpl::SdpToConfig(
    const SdpAudioFormat& format) {
  if (!absl::EqualsIgnoreCase(format.name, "opus") ||
      format.clockrate_hz != kRtpTimestampRateHz || format.num_channels != 2) {
    return absl::nullopt;
  }

  AudioEncoderOpusConfig config;
  config.num_channels = GetChannelCount(format);
  config.frame_size_ms = GetFrameSizeMs(format);
  config.max_playback_rate_hz = GetMaxPlaybackRate(format);
  config.fec_enabled = (GetFormatParameter(format, "useinbandfec") == "1");
  config.dtx_enabled = (GetFormatParameter(format, "usedtx") == "1");
  config.cbr_enabled = (GetFormatParameter(format, "cbr") == "1");
  config.bitrate_bps =
      CalculateBitrate(config.max_playback_rate_hz, config.num_channels,
                       GetFormatParameter(format, "maxaveragebitrate"));
  config.application = config.num_channels == 1
                           ? AudioEncoderOpusConfig::ApplicationMode::kVoip
                           : AudioEncoderOpusConfig::ApplicationMode::kAudio;

  constexpr int kMinANAFrameLength = kANASupportedFrameLengths[0];
  constexpr int kMaxANAFrameLength =
      kANASupportedFrameLengths[arraysize(kANASupportedFrameLengths) - 1];

  // For now, minptime and maxptime are only used with ANA. If ptime is outside
  // of this range, it will get adjusted once ANA takes hold. Ideally, we'd know
  // if ANA was to be used when setting up the config, and adjust accordingly.
  const int min_frame_length_ms =
      GetFormatParameter<int>(format, "minptime").value_or(kMinANAFrameLength);
  const int max_frame_length_ms =
      GetFormatParameter<int>(format, "maxptime").value_or(kMaxANAFrameLength);

  FindSupportedFrameLengths(min_frame_length_ms, max_frame_length_ms,
                            &config.supported_frame_lengths_ms);
  if (!config.IsOk()) {
    RTC_DCHECK_NOTREACHED();
    return absl::nullopt;
  }
  return config;
}

```



这段代码是将SDP音频格式描述转化为Opus音频编码器的配置

函数接受一个SdpAudioFormat类型的参数format，该参数描述了音频格式的相关信息。函数首先检查format是否符合Opus编码器的要求，即名称为"opus"、采样率为kRtpTimestampRateHz、通道数为2，如果不符合，则返回absl::nullopt表示无效配置。

这并不是说明只能48k，双通道，这只是一个约束而已。



函数创建一个AudioEncoderOpusConfig对象config，并根据format中的信息设置config的各个成员变量，包括通道数、帧大小、最大回放速率、前向纠错（fec）是否启用、DTX（Discontinuous Transmission）是否启用、是否启用固定比特率（cbr）、比特率等。



函数通过获取format中的"minptime"和"maxptime"参数值来设置最小帧长度和最大帧长度，并调用FindSupportedFrameLengths函数来确定支持的帧长度范围，并将结果存储在config的supported_frame_lengths_ms成员变量中。



最后，函数检查config是否有效，如果不是有效配置，则触发RTC_DCHECK_NOTREACHED()断言，并返回absl::nullopt。否则，返回配置好的config对象。

> 要修改，我们就修改的是这段代码







```c++
/*
 *  Copyright (c) 2017 The WebRTC project authors. All Rights Reserved.
 *
 *  Use of this source code is governed by a BSD-style license
 *  that can be found in the LICENSE file in the root of the source
 *  tree. An additional intellectual property rights grant can be found
 *  in the file PATENTS.  All contributing project authors may
 *  be found in the AUTHORS file in the root of the source tree.
 */

#include "api/audio_codecs/builtin_audio_encoder_factory.h"

#include <memory>
#include <vector>

#include "api/audio_codecs/L16/audio_encoder_L16.h"
#include "api/audio_codecs/audio_encoder_factory_template.h"
#include "api/audio_codecs/g711/audio_encoder_g711.h"
#include "api/audio_codecs/g722/audio_encoder_g722.h"
#if WEBRTC_USE_BUILTIN_ILBC
#include "api/audio_codecs/ilbc/audio_encoder_ilbc.h"  // nogncheck
#endif
#if WEBRTC_USE_BUILTIN_OPUS
#include "api/audio_codecs/opus/audio_encoder_multi_channel_opus.h"
#include "api/audio_codecs/opus/audio_encoder_opus.h"  // nogncheck
#endif

namespace webrtc {

namespace {

// Modify an audio encoder to not advertise support for anything.
template <typename T>
struct NotAdvertised {
  using Config = typename T::Config;
  static absl::optional<Config> SdpToConfig(
      const SdpAudioFormat& audio_format) {
    return T::SdpToConfig(audio_format);
  }
  static void AppendSupportedEncoders(std::vector<AudioCodecSpec>* specs) {
    // Don't advertise support for anything.
  }
  static AudioCodecInfo QueryAudioEncoder(const Config& config) {
    return T::QueryAudioEncoder(config);
  }
  static std::unique_ptr<AudioEncoder> MakeAudioEncoder(
      const Config& config,
      int payload_type,
      absl::optional<AudioCodecPairId> codec_pair_id = absl::nullopt,
      const FieldTrialsView* field_trials = nullptr) {
    return T::MakeAudioEncoder(config, payload_type, codec_pair_id,
                               field_trials);
  }
};

}  // namespace

rtc::scoped_refptr<AudioEncoderFactory> CreateBuiltinAudioEncoderFactory() {
  return CreateAudioEncoderFactory<

#if WEBRTC_USE_BUILTIN_OPUS
      AudioEncoderOpus, NotAdvertised<AudioEncoderMultiChannelOpus>,
#endif

      AudioEncoderG722,

#if WEBRTC_USE_BUILTIN_ILBC
      AudioEncoderIlbc,
#endif

      AudioEncoderG711, NotAdvertised<AudioEncoderL16>>();
}

}  // namespace webrtc

```

这段代码是用于创建内置音频编码器工厂。

在函数内部，根据预定义的编译选项（如`WEBRTC_USE_BUILTIN_OPUS`和`WEBRTC_USE_BUILTIN_ILBC`），使用`CreateAudioEncoderFactory`模板函数创建音频编码器工厂。

- 如果`WEBRTC_USE_BUILTIN_OPUS`定义为真，则创建Opus编码器(`AudioEncoderOpus`)。
- 如果`WEBRTC_USE_BUILTIN_ILBC`定义为真，则创建iLBC编码器(`AudioEncoderIlbc`)。
- 创建G.722编码器(`AudioEncoderG722`)。
- 创建G.711编码器(`AudioEncoderG711`)。
- 创建L16编码器(`AudioEncoderL16`)。



修改后的代码如下所示：


```c++
rtc::scoped_refptr<AudioEncoderFactory> CreateBuiltinAudioEncoderFactory() {
  return CreateAudioEncoderFactory<

      AudioEncoderOpus>()

      // 注释掉其他编码器
      // AudioEncoderG722,
      // AudioEncoderIlbc,
      // AudioEncoderG711,
      //NotAdvertised<AudioEncoderL16>>();
}

```



*我把这两处修改了以后，编译打包测试后，我没能完成数据包大小的定量测量，从直观上听着感觉音质变差，个人认为起到了作用。*



### 此时，小墩兽同学和我说，在录音时也对采样率有要求，我刚好看了这部分源码，就去仔细找了一下。

audio_device_generic.cc 根据不同设备选择不同的audio_divice

```c++
//录音位置
int32_t AudioDeviceModuleImpl::StartRecording() {
  RTC_LOG(LS_INFO) << __FUNCTION__;
  CHECKinitialized_();
  if (Recording()) {
    return 0;
  }
  audio_device_buffer_.StartRecording();
  int32_t result = audio_device_->StartRecording();
  RTC_LOG(LS_INFO) << "output: " << result;
  RTC_HISTOGRAM_BOOLEAN("WebRTC.Audio.StartRecordingSuccess",
                        static_cast<int>(result == 0));
  return result;
}
```



- 对于android，调用了java层的AudioRecord框架（从C跟回java目前我还没有跟进）

![image-20230707173224504](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230707173224504.png)

- Mac 电脑，使用的是 CoreAudio API，一般情况下使用默认内置的声卡参数 fs=48kHz，stero。

- iOS 一般使用 AudioUnit 框架。

- Windows 电脑，在audiod_device/win/audio_device_core_win.cc下通过CPP代码实现采集。

![image-20230709153957374](https://saladday-figure-bed.oss-cn-chengdu.aliyuncs.com/img/image-20230709153957374.png)

略微看了一下代码。

```c++
int32_t AudioDeviceWindowsCore::InitRecordingDMO() {
  RTC_DCHECK(_builtInAecEnabled);
  RTC_DCHECK(_dmo);

  if (SetDMOProperties() == -1) {
    return -1;
  }

  DMO_MEDIA_TYPE mt = {};
  HRESULT hr = MoInitMediaType(&mt, sizeof(WAVEFORMATEX));
  if (FAILED(hr)) {
    MoFreeMediaType(&mt);
    _TraceCOMError(hr);
    return -1;
  }
  mt.majortype = MEDIATYPE_Audio;
  mt.subtype = MEDIASUBTYPE_PCM;
  mt.formattype = FORMAT_WaveFormatEx;

  // Supported formats
  // nChannels: 1 (in AEC-only mode)
  // nSamplesPerSec: 8000, 11025, 16000, 22050
  // wBitsPerSample: 16
  WAVEFORMATEX* ptrWav = reinterpret_cast<WAVEFORMATEX*>(mt.pbFormat);
  ptrWav->wFormatTag = WAVE_FORMAT_PCM;
  ptrWav->nChannels = 1;
  // 16000 is the highest we can support with our resampler.
  ptrWav->nSamplesPerSec = 16000;
  ptrWav->nAvgBytesPerSec = 32000;
  ptrWav->nBlockAlign = 2;
  ptrWav->wBitsPerSample = 16;
  ptrWav->cbSize = 0;

  // Set the VoE format equal to the AEC output format.
  _recAudioFrameSize = ptrWav->nBlockAlign;
  _recSampleRate = ptrWav->nSamplesPerSec;
  _recBlockSize = ptrWav->nSamplesPerSec / 100;
  _recChannels = ptrWav->nChannels;

  // Set the DMO output format parameters.
  hr = _dmo->SetOutputType(kAecCaptureStreamIndex, &mt, 0);
  MoFreeMediaType(&mt);
  if (FAILED(hr)) {
    _TraceCOMError(hr);
    return -1;
  }

  if (_ptrAudioBuffer) {
    _ptrAudioBuffer->SetRecordingSampleRate(_recSampleRate);
    _ptrAudioBuffer->SetRecordingChannels(_recChannels);
  } else {
    // Refer to InitRecording() for comments.
    RTC_LOG(LS_VERBOSE)
        << "AudioDeviceBuffer must be attached before streaming can start";
  }

  _mediaBuffer = rtc::make_ref_counted<MediaBufferImpl>(_recBlockSize *
                                                        _recAudioFrameSize);

  // Optional, but if called, must be after media types are set.
  hr = _dmo->AllocateStreamingResources();
  if (FAILED(hr)) {
    _TraceCOMError(hr);
    return -1;
  }

  _recIsInitialized = true;
  RTC_LOG(LS_VERBOSE) << "Capture side is now initialized";

  return 0;
}
```

这段代码对windows下DMO的支持进行了初始化，配置了录制的格式和各种参数，按理来说，在android下应该配置和其相似。

如果后期需要对原始录制音频进行改动的话，入手应该也比较容易。

### 第三个工作，配置服务器、创建图形化界面、编译WebRTC，这一步太顺利了，没遇到坑。



## 遇到的困难和解决办法

- 很大的体会，体量大、引用复杂的C++项目，还是需要visual studio.....



## 下周工作

### 项目工程上：

1. 测试修改完编解码的WebRTC是否可用。
2. 开发联调和数据备份功能。

### 持续与组内其他成员沟通，协助完成。



